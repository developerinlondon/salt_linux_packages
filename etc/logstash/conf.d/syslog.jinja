{% set elastic_search = pillar['logstash']['elastic_search_server'] -%}
# logstash confs have three parts:
# log 1) input 2) filter 3) output
# http://logstash.net/docs/1.1.9/configuration 
input {
  tcp {
    port => 5544
    type => "syslog"
  }

  udp {
    port => 5544
    type => "syslog"
  }
}
filter {
  grok {
      type => "syslog"
      pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" ]
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{@source_host}" ]
  }
  syslog_pri {
      type => "syslog"
  }

  date {
      type => "syslog"
  #    syslog_timestamp => [ "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      syslog_timestamp => [ "MMM dd HH:mm:ss" ]
  }

  mutate {
      type => "syslog"
      exclude_tags => "_grokparsefailure"
      replace => [ "@source_host", "%{syslog_hostname}" ]
      replace => [ "@message", "%{syslog_message}" ]
  }
  mutate {
      type => "syslog"
      remove => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
  }
}
output {
    elasticsearch {
        index => "syslog-%{+YYYY.MM.dd}"
        # run with elastic search in the same process or not
        embedded => false
        host => "{{elastic_search}}"
        type => "syslog"
    }
    # for debugging
    #stdout { debug => true debug_format => json }
}
